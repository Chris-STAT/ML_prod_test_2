FROM python:3.8

WORKDIR /opt/ml/code

# Install dependencies
COPY requirements-inference.txt .
RUN pip install --no-cache-dir -r requirements-inference.txt
#RUN pip install multi-model-server sagemaker-inference

# Verify installation of sagemaker_inference by importing a known function or class
# RUN python -c "from sagemaker_inference import model_fn"

# Copy inference code
COPY inference.py .

# Set environment variables
ENV SAGEMAKER_PROGRAM inference.py
ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/model/code

# Use multi-model server as the entry point
# ENTRYPOINT ["torchserve", "--start", "--model-store", "/opt/ml/model/code", "--ncs"]
ENTRYPOINT ["python", "-m", "sagemaker_containers.serve"]


